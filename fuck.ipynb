{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADBench: Anomaly Detection Benchmark\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./data/paper/ADBench.tar.gz'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提取arxiv 源文件\n",
    "import arxiv\n",
    "\n",
    "query_name = input('input your paper here (only arxiv paper is ok)')\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query=query_name,\n",
    "    id_list=[],\n",
    "    max_results=1,\n",
    "    sort_by=arxiv.SortCriterion.Relevance,\n",
    "    sort_order=arxiv.SortOrder.Descending,\n",
    ")\n",
    "\n",
    "paper = next(search.results())\n",
    "print(paper.title)\n",
    "\n",
    "directory = \"./data/paper\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "# Download the archive to a specified directory with a custom filename.\n",
    "paper.download_source(dirpath=\"./data/paper\", filename=f\"{query_name}.tar.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted: 00abstract.tex\n",
      "Extracted: 1intro.tex\n",
      "Extracted: 2related.tex\n",
      "Extracted: 3setting.tex\n",
      "Extracted: 4exp.tex\n",
      "Extracted: 5discussion.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/AUCROC_label_1.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/AUCROC_label_0.25.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/AUCPR_label_0.25.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/20news.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/AUCROC_unsup.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/AUCROC_label_0.75.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/AUCPR_unsup.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/AUCPR_label_0.5.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/datasets.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/AUCPR_label_0.75.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/hyperparameter_grid.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/AUCPR_label_1.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/AUCPR_label_0.1.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/AUCPR_label_0.01.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/AUCROC_label_0.01.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/AUCROC_label_0.5.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/MVTec-AD.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/benchmark.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/AUCROC_label_0.1.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/AUCROC_label_0.05.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/tables/AUCPR_label_0.05.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/dfn.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/abstract_submission.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/3setting.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/1intro.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/main.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/00abstract.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/2related.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/5discussion.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/4exp.tex\n",
      "Extracted: ADBench-NeurIPS-Revision (1)/appendix.tex\n",
      "Extracted: abstract_submission.tex\n",
      "Extracted: appendix.tex\n",
      "Extracted: dfn.tex\n",
      "Extracted: main.tex\n",
      "Extracted: tables/AUCROC_label_1.tex\n",
      "Extracted: tables/AUCROC_label_0.25.tex\n",
      "Extracted: tables/AUCPR_label_0.25.tex\n",
      "Extracted: tables/20news.tex\n",
      "Extracted: tables/AUCROC_unsup.tex\n",
      "Extracted: tables/AUCROC_label_0.75.tex\n",
      "Extracted: tables/AUCPR_unsup.tex\n",
      "Extracted: tables/AUCPR_label_0.5.tex\n",
      "Extracted: tables/datasets.tex\n",
      "Extracted: tables/AUCPR_label_0.75.tex\n",
      "Extracted: tables/hyperparameter_grid.tex\n",
      "Extracted: tables/AUCPR_label_1.tex\n",
      "Extracted: tables/AUCPR_label_0.1.tex\n",
      "Extracted: tables/AUCPR_label_0.01.tex\n",
      "Extracted: tables/AUCROC_label_0.01.tex\n",
      "Extracted: tables/AUCROC_label_0.5.tex\n",
      "Extracted: tables/MVTec-AD.tex\n",
      "Extracted: tables/benchmark.tex\n",
      "Extracted: tables/AUCROC_label_0.1.tex\n",
      "Extracted: tables/AUCROC_label_0.05.tex\n",
      "Extracted: tables/AUCPR_label_0.05.tex\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arxiv cleaner过一遍\n",
    "import tarfile\n",
    "import os\n",
    "def extract_tex_files(dirpath,save_path):\n",
    "    with tarfile.open(dirpath, 'r:gz') as tar:\n",
    "        # 遍历压缩文件中的每个文件/文件夹\n",
    "        for member in tar.getmembers():\n",
    "            # 检查文件是否为 .tex 格式\n",
    "            if member.isreg() and member.name.endswith('.tex'):\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                tar.extract(member, save_path)\n",
    "                print(f\"Extracted: {member.name}\")\n",
    "\n",
    "# 示例用法\n",
    "dirpath = f'./data/paper/{query_name}.tar.gz'\n",
    "save_path = dirpath.split('.tar.gz')[0]\n",
    "extract_tex_files(dirpath, save_path)\n",
    "os.system(f'arxiv_latex_cleaner {save_path} --config cleaner_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrated content has been written to ./data/paper/ADBench_arXiv/integrated_main.tex\n"
     ]
    }
   ],
   "source": [
    "# 处理arXiv 文本内容\n",
    "import os\n",
    "import re\n",
    "\n",
    "def find_main_tex_file(directory_path):\n",
    "    \"\"\"在给定目录中查找包含“main”的.tex文件。\"\"\"\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if \"main\" in file_name and file_name.endswith(\".tex\"):\n",
    "            return file_name\n",
    "    return None\n",
    "\n",
    "def replace_input_with_content(match, directory_path):\n",
    "    \"\"\"替换\\input命令为实际文件内容。\"\"\"\n",
    "    file_name = match.group(1) + \".tex\"\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    return \"\"\n",
    "\n",
    "def integrate_content(directory_path):\n",
    "    main_file_name = find_main_tex_file(directory_path)\n",
    "    if not main_file_name:\n",
    "        raise ValueError(\"主文件未找到。\")\n",
    "\n",
    "    main_file_path = os.path.join(directory_path, main_file_name)\n",
    "    with open(main_file_path, 'r', encoding='utf-8') as f:\n",
    "        main_content = f.read()\n",
    "        \n",
    "    # 删除包含 \"appendix\" 的 \\input{} 命令\n",
    "    main_content = re.sub(r\"\\\\input\\{.*?appendix.*?\\}\", \"\", main_content)\n",
    "    \n",
    "    # 删除 \\section{} 和 \\subsection{} 中包含 \"appendix\" 的部分及其之后的内容，直到下一个 \\section{} 或 \\subsection{}\n",
    "    main_content = re.sub(r\"(\\\\section\\{.*?appendix.*?\\}.*?)(?=\\\\section|\\Z)\", \"\", main_content, flags=re.DOTALL|re.IGNORECASE)\n",
    "    main_content = re.sub(r\"(\\\\subsection\\{.*?appendix.*?\\}.*?)(?=\\\\section|\\\\subsection|\\Z)\", \"\", main_content, flags=re.DOTALL|re.IGNORECASE)\n",
    "    pattern = re.compile(r\"\\\\section\\*?\\{.*?acknowledgement.*?\\}\", re.IGNORECASE)\n",
    "    main_content = re.sub(pattern, '', main_content)\n",
    "\n",
    "    # 替换\\input命令\n",
    "    pattern = r\"\\\\input\\{([^\\}]+)\\}\"\n",
    "    main_content = re.sub(pattern, lambda m: replace_input_with_content(m, directory_path), main_content)\n",
    "    main_content = re.sub(r'\\\\label\\{.*?\\}', '', main_content)\n",
    "    main_content = re.sub(r'\\\\input\\{.*?\\}', '', main_content)\n",
    "    commands_to_remove = [\n",
    "        r'\\\\small',\n",
    "        r'\\\\clearpage',\n",
    "        r'\\\\newpage',\n",
    "        r'\\\\normalsize',\n",
    "        r'\\\\maketitle'\n",
    "    ]\n",
    "    for cmd in commands_to_remove:\n",
    "        main_content = re.sub(cmd, '', main_content)\n",
    "    return main_content\n",
    "\n",
    "def extract_content_between_document_tags(main_content):\n",
    "    pattern = r\"\\\\begin\\{document\\}(.*?)\\\\end\\{document\\}\"\n",
    "    matches = re.search(pattern, main_content, re.DOTALL)\n",
    "    if matches:\n",
    "        return matches.group(1).strip()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def remove_empty_and_single_symbol_lines(content):\n",
    "    # 使用正则表达式分割内容为行\n",
    "    lines = content.split('\\n')\n",
    "    # 过滤出非空行以及长度超过1的行\n",
    "    cleaned_lines = [line for line in lines if line.strip() and len(line.strip()) > 1]\n",
    "    # 合并过滤后的行并返回\n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "def remove_figure_environment(result_content):\n",
    "    pattern = r'\\\\begin\\{figure\\*?(\\[[^\\]]*\\])?\\}.*?\\\\end\\{figure\\*?\\}'\n",
    "    result_content = re.sub(pattern, '', result_content, flags=re.DOTALL)\n",
    "    return result_content\n",
    "\n",
    "def remove_newlines_within_sections(content):\n",
    "    # 将latex命令存储到临时变量，并用一个占位符替换\n",
    "    placeholder = \"%%LATEXCOMMANDPLACEHOLDER%%\"\n",
    "    commands = re.findall(r'\\\\[a-zA-Z]+\\{[^\\}]*\\}', content)\n",
    "    for command in commands:\n",
    "        content = content.replace(command, placeholder + command + placeholder)\n",
    "\n",
    "    def replace_newlines_in_match(match):\n",
    "        # 取得匹配到的内容\n",
    "        section_content = match.group(0)\n",
    "        # 替换内容中的换行，但不替换冒号后的换行\n",
    "        return re.sub(r'(?<!:)\\n', ' ', section_content)\n",
    "\n",
    "    # 对于 \\section 和 \\subsection 的内容替换换行符\n",
    "    pattern = r'(\\\\section\\{.*?\\}.*?)(?=(\\\\section|\\\\subsection|\\Z))'\n",
    "    content = re.sub(pattern, replace_newlines_in_match, content, flags=re.DOTALL)\n",
    "    \n",
    "    pattern = r'(\\\\subsection\\{.*?\\}.*?)(?=(\\\\section|\\\\subsection|\\Z))'\n",
    "    content = re.sub(pattern, replace_newlines_in_match, content, flags=re.DOTALL)\n",
    "    \n",
    "    # 恢复之前存储的latex命令\n",
    "    for command in commands:\n",
    "        content = content.replace(placeholder + command + placeholder, command)\n",
    "    \n",
    "    return content\n",
    "\n",
    "def add_newlines_before_commands(content):\n",
    "    # 删除多余的换行，保留冒号后的换行\n",
    "    content = re.sub(r'(?<!:)\\n', ' ', content)\n",
    "\n",
    "    # 在特定命令前添加换行\n",
    "    commands_to_start_with_newline = [\n",
    "        r'\\\\section{',\n",
    "        r'\\\\subsection{',\n",
    "        r'\\\\begin{'\n",
    "    ]\n",
    "\n",
    "    for cmd in commands_to_start_with_newline:\n",
    "        content = re.sub(cmd, '\\n' + cmd, content)\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "\n",
    "# 指定存放TeX文件的目录路径\n",
    "directory_path = save_path+'_arXiv'\n",
    "result_content = integrate_content(directory_path)\n",
    "result_content = extract_content_between_document_tags(result_content)\n",
    "result_content = remove_empty_and_single_symbol_lines(result_content)\n",
    "result_content = remove_figure_environment(result_content)\n",
    "result_content = remove_newlines_within_sections(result_content)\n",
    "result_content = add_newlines_before_commands(result_content)\n",
    "\n",
    "# 将结果写入新的文件\n",
    "output_file_path = os.path.join(directory_path, \"integrated_main.tex\")\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Full paper contents:\")\n",
    "    f.write(result_content)\n",
    "\n",
    "print(f\"Integrated content has been written to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /Users/ppwang/miniforge3/lib/python3.10/site-packages (1.9.2)\n",
      "Requirement already satisfied: nltk in /Users/ppwang/miniforge3/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: pandas in /Users/ppwang/miniforge3/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: imageio in /Users/ppwang/miniforge3/lib/python3.10/site-packages (2.31.5)\n",
      "Requirement already satisfied: selenium in /Users/ppwang/miniforge3/lib/python3.10/site-packages (4.9.0)\n",
      "Requirement already satisfied: tqdm in /Users/ppwang/miniforge3/lib/python3.10/site-packages (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/ppwang/.local/lib/python3.10/site-packages (from wordcloud) (1.23.4)\n",
      "Requirement already satisfied: matplotlib in /Users/ppwang/.local/lib/python3.10/site-packages (from wordcloud) (3.6.2)\n",
      "Requirement already satisfied: pillow in /Users/ppwang/.local/lib/python3.10/site-packages (from wordcloud) (9.3.0)\n",
      "Requirement already satisfied: click in /Users/ppwang/.local/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/ppwang/miniforge3/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ppwang/miniforge3/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ppwang/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ppwang/.local/lib/python3.10/site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/ppwang/miniforge3/lib/python3.10/site-packages (from selenium) (2022.9.24)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in /Users/ppwang/miniforge3/lib/python3.10/site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/ppwang/miniforge3/lib/python3.10/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/ppwang/miniforge3/lib/python3.10/site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ppwang/miniforge3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /Users/ppwang/miniforge3/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: attrs>=20.1.0 in /Users/ppwang/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: idna in /Users/ppwang/miniforge3/lib/python3.10/site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sniffio in /Users/ppwang/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: outcome in /Users/ppwang/miniforge3/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/ppwang/miniforge3/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/ppwang/miniforge3/lib/python3.10/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/ppwang/miniforge3/lib/python3.10/site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/ppwang/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ppwang/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ppwang/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ppwang/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ppwang/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ppwang/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/ppwang/miniforge3/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ppwang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ppwang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ppwang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ppwang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 爬取OpenReview 审稿意见\n",
    "! pip install wordcloud nltk pandas imageio selenium tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openreview.net/forum?id=foA_SFQ9zo0\n"
     ]
    }
   ],
   "source": [
    "# get openreview results\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.edge.service import Service\n",
    "\n",
    "driver = webdriver.Chrome(executable_path='/usr/local/bin/chromedriver')\n",
    "\n",
    "# automate search query_name in openreview and get first link.\n",
    "search_link = f'https://openreview.net/search?term={query_name}&group=all&content=all&source=all'\n",
    "driver.get(search_link)\n",
    "\n",
    "# XPath for the link\n",
    "xpath = '//*[@id=\"content\"]/div/div/ul/li[1]/div/h4/a[1]'\n",
    "\n",
    "# Waiting for the link to appear\n",
    "link_element = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.XPATH, xpath))\n",
    ")\n",
    "\n",
    "review_path = link_element.get_attribute(\"href\")\n",
    "print(review_path)\n",
    "\n",
    "#link = input(\"link\")\n",
    "driver.get(review_path)\n",
    "xpath = '//div[@id=\"note_children\"]//span[@class=\"note_content_value\"]/..'\n",
    "cond = EC.presence_of_element_located((By.XPATH, xpath))\n",
    "WebDriverWait(driver, 60).until(cond)\n",
    "\n",
    "elems = driver.find_elements_by_xpath(xpath)\n",
    "assert len(elems), 'empty ratings'\n",
    "ratings = [int(x.text.split(': ')[1]) for x in elems if x.text.startswith('Rating:')]\n",
    "decisions = [x.text.split(': ')[1] for x in elems if x.text.startswith('Decision:')]\n",
    "summary_elems = driver.find_elements_by_xpath(\"//span[contains(text(), 'Summary And Contributions:')]/following-sibling::span[@class='note_content_value markdown-rendered']\")\n",
    "summaries = [' '.join(x.text for x in elem.find_elements_by_xpath(\".//p\")) for elem in summary_elems]\n",
    "\n",
    "strengths_elems = driver.find_elements_by_xpath(\"//span[contains(text(), 'Strengths:')]/following-sibling::span[@class='note_content_value markdown-rendered']\")\n",
    "strengths_list = [';'.join(x.text for x in elem.find_elements_by_xpath(\".//li\")) for elem in strengths_elems]\n",
    "\n",
    "weaknesses_elems = driver.find_elements_by_xpath(\"//span[contains(text(), 'Weaknesses:')]/following-sibling::span[@class='note_content_value markdown-rendered']\")\n",
    "weaknesses_list = [';'.join(x.text for x in elem.find_elements_by_xpath(\".//li\")) for elem in weaknesses_elems]\n",
    "#print(ratings, decisions, summaries, strengths_list, weaknesses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ratings Decisions                          Summary and Contributions  \\\n",
      "0        7    Accept  This paper introduces ADBench, a tabular anoma...   \n",
      "1        8    Accept  The authors propose a comprehensive Anomaly De...   \n",
      "2        6    Accept  This paper presents ADBench, a comprehensive t...   \n",
      "3        7    Accept  The paper proposes a comprehensive benchmark f...   \n",
      "4        7    Accept  Although there are many existing AD benchmarks...   \n",
      "5        6    Accept  This paper provides a detailed and thorough be...   \n",
      "\n",
      "                                           Strengths  \\\n",
      "0  Large, well-designed benchmark for tabular ano...   \n",
      "1  The authors have proposed a large collection o...   \n",
      "2  ADBench has a large algorithm collection with ...   \n",
      "3  the ADBench proposed in the paper includes the...   \n",
      "4  This paper provides a comprehensive AD benchma...   \n",
      "5                                                      \n",
      "\n",
      "                                          Weaknesses  \n",
      "0  At the end of the day, it doesn't feel like th...  \n",
      "1  You have not included datasets tailored for an...  \n",
      "2                                                     \n",
      "3  The paper depends on 1 single method to extrac...  \n",
      "4                                                     \n",
      "5  What is the domain gap between the real-world ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Ratings': ratings,\n",
    "    'Decisions': decisions*len(ratings),\n",
    "    'Summary and Contributions': summaries,\n",
    "    'Strengths': strengths_list,\n",
    "    'Weaknesses': weaknesses_list\n",
    "}\n",
    "\n",
    "# 创建数据框架\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 保存为CSV文件\n",
    "df.to_csv(f'{directory_path}/reviews_data.csv', index=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将paper 和 review 结合\n",
    "# 1. 读取 .tex 文件\n",
    "query_name='ADBench'\n",
    "dirpath = f'./data/paper/{query_name}.tar.gz'\n",
    "save_path = dirpath.split('.tar.gz')[0]\n",
    "directory_path = save_path+'_arXiv'\n",
    "with open(f'{directory_path}/integrated_main.tex', 'r', encoding='utf-8') as tex_file:\n",
    "    tex_content = tex_file.read()\n",
    "\n",
    "# 2. 读取 .csv 文件\n",
    "with open('reviews_data.csv', 'r', encoding='utf-8') as csv_file:\n",
    "    csv_content = csv_file.read()\n",
    "\n",
    "# 3. 合并内容\n",
    "combined_content = \"作为人工智能顶级会议(NeurIPS, ICLR)的资深审稿人，请根据以下提供的论文和审稿意见，提供一个论文分享PPT的内容概要，要求PPT不超过10页。\\n\\n\" + tex_content + '\\n\\nOpenReview contents:\\n' + csv_content\n",
    "\n",
    "# 4. 将合并的内容写入一个 .txt 文件\n",
    "with open(f'{directory_path}/output.txt', 'w', encoding='utf-8') as txt_file:\n",
    "    txt_file.write(combined_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
